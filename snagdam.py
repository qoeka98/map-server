import streamlit as st
from huggingface_hub import InferenceClient
import re
import time

# âœ… ì‚¬ìš©ì ì…ë ¥ í•„í„°ë§ í•¨ìˆ˜
def clean_input(text):
    text = re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE)
    return text.strip()

# âœ… ì§€ì§„ ê´€ë ¨ ì§ˆë¬¸ í•„í„°ë§ í•¨ìˆ˜
def is_earthquake_related(text):
    earthquake_keywords = ["ì§€ì§„", "ì§„ë„", "ì§„ì•™", "ì§€ì§„ ëŒ€ë¹„", "ì“°ë‚˜ë¯¸", "ëŒ€í”¼ì†Œ"]
    return any(keyword in text for keyword in earthquake_keywords)

# âœ… Hugging Face API í† í° ê°€ì ¸ì˜¤ê¸°
def get_huggingface_token():
    return st.secrets.get("HUGGINGFACE_API_TOKEN")

# âœ… AI ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜
def run_sangdam():
    st.markdown("<h1 style='text-align: center; color: #007bff;'>ğŸŒ ì§€ì§„ ëŒ€ë¹„ AI ì±—ë´‡</h1>", unsafe_allow_html=True)

    # âœ… CSS ìŠ¤íƒ€ì¼ ì¶”ê°€ (ìœ ì €ì™€ AI ë©”ì‹œì§€ êµ¬ë¶„)
    st.markdown("""
        <style>
            .chat-container {
                display: flex;
                align-items: flex-start;
                margin-bottom: 10px;
            }
            .chat-icon {
                width: 40px;
                height: 40px;
                border-radius: 50%;
                margin-right: 10px;
            }
            .user-message {
                background-color: #d4f8c4;
                padding: 10px;
                border-radius: 12px;
                margin-bottom: 10px;
                font-weight: bold;
                max-width: 80%;
            }
            .ai-message {
                background-color: #fff5cc;
                padding: 10px;
                border-radius: 12px;
                margin-bottom: 10px;
                max-width: 80%;
            }
        </style>
    """, unsafe_allow_html=True)

    # âœ… ì‚¬ìš© ë°©ë²• ì•ˆë‚´
    st.info(
        """
        ğŸ” **ì´ ì±—ë´‡ì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.**  
        - âœ… ì‹¤ì‹œê°„ ì§€ì§„ ëŒ€ë¹„ ë° ì•ˆì „ ëŒ€ì±… ì œê³µ  
        - âœ… ì§€ì§„ ë°œìƒ ì‹œ í–‰ë™ ìš”ë ¹ ì•ˆë‚´  
        - âœ… ë‚´ì§„ ì„¤ê³„ ë° ì§€ì§„ ì˜ˆì¸¡ ê´€ë ¨ ì •ë³´ ì œê³µ  
        - âœ… ì“°ë‚˜ë¯¸ ë° ê¸´ê¸‰ ëŒ€í”¼ì†Œ ì •ë³´ ì•ˆë‚´  

        **ğŸ“ ì‚¬ìš© ë°©ë²•:**  
        1ï¸âƒ£ ì•„ë˜ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.  
        2ï¸âƒ£ AIê°€ ì§€ì§„ ëŒ€ë¹„ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.  
        3ï¸âƒ£ ì›í•˜ëŠ” ì§ˆë¬¸ì„ ë°˜ë³µ ì…ë ¥í•˜ì—¬ ìƒë‹´í•˜ì„¸ìš”.  
        """
    )

    # âœ… ì˜ˆì‹œ ì§ˆë¬¸ í‘œì‹œ
    with st.expander("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸ ë³´ê¸°"):
        st.markdown(
            """
            - ìµœê·¼ ì§€ì§„ ì •ë³´ëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?
            - ì§€ì§„ ë°œìƒ ì‹œ ê°€ì¥ ì•ˆì „í•œ ì¥ì†ŒëŠ”?
            - ì§€ì§„ ëŒ€ë¹„ë¥¼ ìœ„í•´ ì–´ë–¤ ë¬¼í’ˆì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?  
            - ë‚´ì§„ ì„¤ê³„ê°€ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”? 
            - ì“°ë‚˜ë¯¸ ê²½ë³´ê°€ ë°œë ¹ë˜ë©´ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ë‚˜ìš”? 
            """,
            unsafe_allow_html=True
        )

    token = get_huggingface_token()
    client = InferenceClient(model="google/gemma-2-9b-it", api_key=token)

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì§€ì§„ ëŒ€ë¹„ ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"}
        ]

    for message in st.session_state.messages:
        role = message["role"]
        message_content = message["content"]

        icon_url = "https://cdn-icons-png.flaticon.com/512/1144/1144760.png" if role == "user" else "https://cdn-icons-png.flaticon.com/512/4712/4712034.png"
        message_class = "user-message" if role == "user" else "ai-message"

        st.markdown(
            f"""
            <div class="chat-container">
                <img src="{icon_url}" class="chat-icon">
                <div class="{message_class}">{message_content}</div>
            </div>
            """,
            unsafe_allow_html=True
        )

    chat = st.chat_input("ì§€ì§„ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!", key="chat_input")

    if chat:
        clean_chat = clean_input(chat)

        if not is_earthquake_related(clean_chat):
            response = "âŒ ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ì§„ ê´€ë ¨ ìƒë‹´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        else:
            # âœ… AI ì‘ë‹µ ìš”ì²­ (Gemma ëª¨ë¸ ì‚¬ìš©)
            system_prompt = '''
            ë„ˆëŠ” ì§€ì§„ ëŒ€ë¹„ ë° ìì—°ì¬í•´ ì „ë¬¸ê°€ AIì•¼.  
            ì‚¬ìš©ìì—ê²Œ ì§€ì§„ ëŒ€ë¹„, ëŒ€í”¼ ìš”ë ¹, ê¸´ê¸‰ ìƒí™© í–‰ë™ ì§€ì¹¨, ë‚´ì§„ ì„¤ê³„, ì§€ì§„ ì˜ˆì¸¡ ê¸°ìˆ , ì‹¬ë¦¬ì  ëŒ€ì²˜ ë°©ë²•ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´.
            '''

            full_prompt = system_prompt + "\n\n" + clean_chat

            with st.spinner("AIê°€ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
                time.sleep(1)  # ìŠ¤í•€ë„ˆê°€ í‘œì‹œë˜ë„ë¡ ì ì‹œ ëŒ€ê¸°
                response = client.text_generation(prompt=full_prompt, max_new_tokens=520)

        st.session_state.messages.append({"role": "user", "content": clean_chat})
        st.session_state.messages.append({"role": "assistant", "content": response})
