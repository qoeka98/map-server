import streamlit as st
import requests
import re
from datetime import datetime
import pytz  # âœ… ì˜¬ë°”ë¥¸ timezone ì‚¬ìš©
from bs4 import BeautifulSoup
import feedparser

# âœ… í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì • (UTC â†’ KST ë³€í™˜)
KST = pytz.timezone('Asia/Seoul')
UTC = pytz.utc  # âœ… UTC ì •ì˜

def clean_html_tags(text):
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    text = re.sub(r"<[^>]*>", "", text)  # <b> ê°™ì€ HTML íƒœê·¸ ì œê±°
    text = text.replace("&quot;", '"').replace("&amp;", "&")  # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
    return text.strip()

def format_date_korean(date_str):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë‚ ì§œë¥¼ 'YYYYë…„ Mì›” Dì¼' í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° datetime ê°ì²´ ë°˜í™˜"""
    try:
        date_obj = datetime.strptime(date_str.replace(".", "-").strip(), "%Y-%m-%d")
        return date_obj, f"{date_obj.year}ë…„ {date_obj.month}ì›” {date_obj.day}ì¼"
    except ValueError:
        return None, date_str  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë˜ ë¬¸ìì—´ ìœ ì§€

def get_naver_earthquake_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìµœì‹  ì§€ì§„ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    query = "ì§€ì§„ OR ê°•ì§„ OR ì—¬ì§„ OR ì“°ë‚˜ë¯¸ OR í•´ì¼ OR ê·œëª¨ OR ì§„ì•™ OR í”¼í•´"
    url = f"https://search.naver.com/search.naver?where=news&query={query}&sort=1"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    news_list = []
    news_items = soup.select("div.news_area")

    for item in news_items[:15]:  
        title = item.select_one("a.news_tit").text
        link = item.select_one("a.news_tit")["href"]
        source = item.select_one("a.info.press").text.strip()
        pub_date = item.select_one("span.info").text.strip()
        
        date_obj, pub_date_korean = format_date_korean(pub_date)

        if date_obj:
            news_list.append({
                "title": title,
                "source": source,
                "link": link,
                "pub_date": pub_date_korean,
                "date_obj": date_obj  # âœ… datetime ê°ì²´ ì €ì¥
            })

    return news_list

def get_google_earthquake_news():
    """êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ ìµœì‹  ì§€ì§„ ê´€ë ¨ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    rss_url = "https://news.google.com/rss/search?q=ì§€ì§„&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)

    news_list = []
    for entry in feed.entries[:15]:  
        pub_date_obj = datetime(*entry.published_parsed[:6])
        pub_date_obj = UTC.localize(pub_date_obj).astimezone(KST)
        pub_date_korean = f"{pub_date_obj.year}ë…„ {pub_date_obj.month}ì›” {pub_date_obj.day}ì¼"

        news_list.append({
            "title": entry.title,
            "link": entry.link,
            "pub_date": pub_date_korean,
            "date_obj": pub_date_obj  # âœ… datetime ê°ì²´ ì €ì¥
        })
    
    return news_list

def run_news():
    st.title("ğŸŒ ì‹¤ì‹œê°„ ì§€ì§„ ë‰´ìŠ¤")

    if st.button("ğŸ”„ ìµœì‹  ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°"):
        st.rerun()

    st.write(f"### ğŸ“° 2025ë…„ ìµœì‹  ì§€ì§„ ë‰´ìŠ¤")

    naver_news = get_naver_earthquake_news()
    google_news = get_google_earthquake_news()
    
    news_articles = sorted(naver_news + google_news, key=lambda x: x['date_obj'], reverse=True)
    
    if not news_articles:
        st.write("âŒ ê´€ë ¨ ì§€ì§„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì‹  ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.)")
    else:
        for news in news_articles:
            with st.expander(f"ğŸ“° [{news.get('pub_date', 'ë‚ ì§œ ì—†ìŒ')}] {news['title']}"):
                st.write(f"ğŸ”— [ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({news['link']})")

if __name__ == "__main__":
    run_news()