import streamlit as st
import requests
import re
from datetime import datetime, timedelta, timezone
from pytz import timezone
from bs4 import BeautifulSoup
import feedparser

# âœ… í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì • (UTC â†’ KST ë³€í™˜)
KST = timezone('Asia/Seoul')

def clean_html_tags(text):
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    text = re.sub(r"<[^>]*>", "", text)  # <b> ê°™ì€ HTML íƒœê·¸ ì œê±°
    text = text.replace("&quot;", '"').replace("&amp;", "&")  # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
    return text.strip()

def get_naver_earthquake_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìµœì‹  ì§€ì§„ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    query = "ì§€ì§„ OR ê°•ì§„ OR ì—¬ì§„ OR ì“°ë‚˜ë¯¸ OR í•´ì¼ OR ê·œëª¨ OR ì§„ì•™ OR í”¼í•´"
    url = f"https://search.naver.com/search.naver?where=news&query={query}&sort=1"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    news_list = []
    news_items = soup.select("div.news_area")  # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ì˜ì—­

    today = datetime.now(KST).strftime('%Y')  # âœ… í˜„ì¬ ì—°ë„ í•„í„°ë§

    for item in news_items[:10]:  # âœ… ìµœì‹  ë‰´ìŠ¤ 10ê°œ ê°€ì ¸ì˜¤ê¸°
        title = item.select_one("a.news_tit").text
        link = item.select_one("a.news_tit")["href"]
        source = item.select_one("a.info.press").text.strip()
        pub_date = item.select_one("span.info").text.strip()
        
        # âœ… ì—°ë„ í•„í„°ë§: 2025ë…„ ê¸°ì‚¬ë§Œ í¬í•¨
        if today in pub_date and any(keyword in title for keyword in ["ì§€ì§„", "ê°•ì§„", "ì—¬ì§„", "ì“°ë‚˜ë¯¸", "í•´ì¼", "ê·œëª¨", "ì§„ì•™", "í”¼í•´"]):
            news_list.append({
                "title": title,
                "source": source,
                "link": link,
                "pub_date": pub_date
            })

    return news_list

def get_google_earthquake_news():
    """êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ ìµœì‹  ì§€ì§„ ê´€ë ¨ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    rss_url = "https://news.google.com/rss/search?q=ì§€ì§„&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)

    news_list = []
    for entry in feed.entries[:5]:  # âœ… ìµœì‹  ë‰´ìŠ¤ 5ê°œ ê°€ì ¸ì˜¤ê¸°
        news_list.append({
            "title": entry.title,
            "link": entry.link,
            "pub_date": entry.published
        })
    
    return news_list

# âœ… 4. Streamlit UI ì„¤ì •
def run_news():
    st.title("ğŸŒ ì‹¤ì‹œê°„ ì§€ì§„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")

    # âœ… 5. "ìƒˆë¡œê³ ì¹¨" ë²„íŠ¼ ì¶”ê°€
    if st.button("ğŸ”„ ìµœì‹  ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°"):
        st.rerun()

    # âœ… 6. ìµœê·¼ ë‰´ìŠ¤ í‘œì‹œ
    st.write(f"### ğŸ“° 2025ë…„ ìµœì‹  ì§€ì§„ ë‰´ìŠ¤ (ë„¤ì´ë²„ & êµ¬ê¸€)")

    naver_news = get_naver_earthquake_news()
    google_news = get_google_earthquake_news()
    
    news_articles = naver_news + google_news
    
    if not news_articles:
        st.write("âŒ ê´€ë ¨ ì§€ì§„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì‹  ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.)")
    else:
        for news in news_articles:
            with st.expander(f"ğŸ“° [{news.get('pub_date', 'ë‚ ì§œ ì—†ìŒ')}] {news['title']}"):
                st.write(f"ğŸ”— [ì›ë¬¸ ë³´ê¸°]({news['link']})")
